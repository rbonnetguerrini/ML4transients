# Co-teaching training configuration with optimized hyperparameters
data:
  path: "/sps/lsst/groups/transients/HSC/fouchez/raphael/data/rc2_norm"
  visits: null
  test_size: 0.2
  val_size: 0.1

training:
  trainer_type: "coteaching"
  epochs: 1500
  learning_rate: 0.001
  batch_size: 128                       # Optimized from Bayesian search
  num_iter_per_epoch: 400
  epoch_decay_start: 80
  num_workers: 4
  output_dir: "/sps/lsst/groups/transients/HSC/fouchez/raphael/training/coteaching_optimized"
  
  forget_rate_0: 0.0073                 # Optimized: 0.007254591234841138
  forget_rate_1: 0.0070                 # Optimized: 0.0070297266812162675
  num_gradual: 7                        # Optimized
  exponent: 1
  
  use_tensorboard: true
  tensorboard_log_dir: "runs"
  experiment_name: "coteaching_optimized"
  log_interval: 100
  
  early_stopping:
    enabled: true
    monitor: "loss"
    mode: "min"
    patience_lr_changes: 5
    min_delta: 0.0
    
  bayes_search:
    enabled: false
    
  model_params:
    input_shape: [30, 30, 1]
    num_classes: 2
    num_conv_blocks: 4                  # Optimized: 4 conv blocks
    filters_1: 16                       # Optimized
    filters_2: 64                       # Optimized
    filters_3: 96                       # Optimized
    filters_4: 384                      # Optimized
    dropout_1: 0.25                     # Keep default
    dropout_2: 0.25                     # Keep default
    dropout_3: 0.5                      # Keep default
    units: 256                          # Optimized

random_state: 42
