# Co-teaching training configuration with optimized hyperparameters
data:
  path: "/sps/lsst/groups/transients/HSC/fouchez/raphael/data/rc2_coadd_v2"
  visits: null
  test_size: 0.2
  val_size: 0.1
  cutout_types: ['diff', 'coadd'] 
  
training:
  trainer_type: "coteaching"
  epochs: 1500
  learning_rate: 0.0027298633537170608
  batch_size: 64                      
  num_iter_per_epoch: 400
  epoch_decay_start: 80
  num_workers: 4
  output_dir: "/sps/lsst/groups/transients/HSC/fouchez/raphael/training/coadd_coteaching_optimized"
  
  forget_rate_0: 0.16                 
  forget_rate_1: 0.02                
  num_gradual: 13                       
  exponent: 1
  
  use_tensorboard: true
  tensorboard_log_dir: "runs"
  experiment_name: "coteaching"
  log_interval: 100
  
  early_stopping:
    enabled: true
    monitor: "fnr"                      # Monitor False Negative Rate instead of loss
    mode: "min"                         # Want to minimize FNR
    patience: 80
    min_delta: 0.001                    
    
  model_params:
    input_shape: [30, 30, 2]
    num_classes: 2
    num_conv_blocks: 3                  
    filters_1: 16                       
    filters_2: 64                       
    filters_3: 64                       
    filters_4: 192                      
    dropout_1: 0.25                    
    dropout_2: 0.4                     
    dropout_3: 0.55
    in_channels: 2                              
    units: 448                          

random_state: 42
