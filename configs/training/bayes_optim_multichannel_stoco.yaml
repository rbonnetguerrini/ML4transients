# Bayesian hyperparameter optimization with multi-channel inputs (diff + coadd)
data:
  path: "/sps/lsst/groups/transients/HSC/fouchez/raphael/data/rc2_89570"
  visits: null
  test_size: 0.2
  val_size: 0.1
  cutout_types: ['diff', 'coadd']  # Use both diff and coadd as 2-channel input

training:
  trainer_type: "stochastic_coteaching"  # Can also use "ensemble" or "coteaching"
  epochs: 150  # Max epochs per trial
  learning_rate: 0.001
  batch_size: 128
  epoch_decay_start: 80
  num_workers: 4
  output_dir: "/sps/lsst/groups/transients/HSC/fouchez/raphael/training/bayes_optim_multichannel_stoco_89570"
  
  use_tensorboard: true
  tensorboard_log_dir: "runs"
  experiment_name: "bayes_optim_multichannel_stoco_89570"
  log_interval: 50

 # Stochastic co-teaching specific parameters
  alpha: 32                             # Beta distribution alpha 
  beta: 4                               # Beta distribution beta 
  num_gradual: 10                       # Gradual warmup period (tp_gradual)
  delay: 0                              # Delay before starting rejection 
  exponent: 1                           # Exponent for schedule
  clip: [0.01, 0.99]                    # Min/max clipping for random thresholds
  seed: 42
  
  bayes_search:
    enabled: true
    monitor: "fnr"  # Metric to optimize: 'fnr', 'loss', or 'accuracy'
    direction: minimize  # Minimize FNR (False Negative Rate)
    prune_metric: "loss"  # Metric for pruning: 'loss' or 'accuracy' (more stable than FNR for early stopping)
    n_trials: 50  # Number of trials to run
    max_epochs: 150  # Max epochs per trial
    prune: true  # Enable pruning of unpromising trials
    hpo_train_fraction: 0.3  # Use 30% of training data during HPO
    
    # Parameters to optimize
    # Simplified strategy: geometric progression for filters (F, 2F, 4F)
    # and scaled dropout rates (0.5*DR, 0.5*DR, DR)
    params:
      batch_size:
        type: int
        low: 64
        high: 256
        step: 64
      
      learning_rate:
        type: float
        low: 0.0001
        high: 0.01
        log: true  # Log scale for learning rate
      
      # Base filter width F - filters will be (F, 2F, 4F)
      model_params.base_filters:
        type: int
        low: 16
        high: 64
        step: 16
      
      # Base dropout rate DR - dropouts will be (0.5*DR, 0.5*DR, DR)
      model_params.base_dropout:
        type: float
        low: 0.2
        high: 0.6
        step: 0.05

      model_params.units:
        type: int
        low: 64
        high: 512
        step: 64
    
  # Base model parameters (will be overridden by Bayesian optimization)
  # Fixed architecture: 3 convolutional blocks with geometric filter progression
  model_params:
    input_shape: [30, 30, 2]  # 2 channels (diff + coadd)
    num_classes: 2
    num_conv_blocks: 3  # Fixed to 3 blocks
    base_filters: 32  # F - filters will be (F, 2F, 4F) = (32, 64, 128)
    filters_1: 32  # Will be overridden by base_filters
    filters_2: 64  # Will be overridden by 2*base_filters
    filters_3: 128  # Will be overridden by 4*base_filters
    filters_4: 256  # Not used with 3 blocks
    base_dropout: 0.5  # DR - dropouts will be (0.5*DR, 0.5*DR, DR)
    dropout_1: 0.25  # Will be overridden by 0.5*base_dropout
    dropout_2: 0.25  # Will be overridden by 0.5*base_dropout
    dropout_3: 0.5  # Will be overridden by base_dropout
    units: 128
    in_channels: 2  # Explicitly set 2 input channels

random_state: 42
